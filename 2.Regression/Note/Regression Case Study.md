## Regression: Case Study

### 回归任务示例

**股票预测**：输入过去的股票信息，给出某股票的未来价格；

**无人车**：输入路况，给出驾驶路线；

**推荐系统**：给出使用者A与商品B，给出购买可能性。

### 一个实例：预测Pokemon的战斗力（CP值）

通过将Pokemon的体重等参数作线性变换，得到一个预测值y，表示为其战斗力。

***Step1: Model***

从函数集合中寻找一种$y=b+w \cdot x_{cp}$。此处，使用到的函数是一种线性模型：
$$
y = b + \sum w_i x_i
$$
其中，$x_i$是输入向量的一个维度特征，$w_i$称为权重weight，$b$称为偏置bias。

***Step2: Goodness of Function***

搜集10只pokemon的数据$(x^i, \hat{y}^i)$，进行训练。此时通过设计一个函数，令其输入一个函数，输出其好坏，如：
$$
L(f) = L(w, b) = \sum_{n=1}^{10}(\hat{y}^n - (b + w \cdot x_{cp}^n))^2
$$
函数中$\hat{y}$表示真实值，因此该损失函数含义为估计误差与真实值的平方损失。

***Step 3: Best Function***

从定义好的函数中挑选一个效果最好的，即确定其参数。
$$
\begin{align*}
f^* &= arg\min_f L(f)
\\
w^*, b^* &= arg\min_{w, b} L(w, b)
\end{align*}
$$
***Gradient Descent***

当函数可微分时，通过梯度下降的方式寻找损失函数最小的参数。

<img src="/Users/LightningX/Learning/ML2020/2.Regression/Note/截屏2020-08-19 16.23.08.png" alt="截屏2020-08-19 16.23.08" style="zoom:33%;" />

想象函数图像如山谷，在寻找的过程中，设定初始值，得到当前的梯度，观察梯度的方向，向其相反方向移动取值。例如梯度为负数，则有$w^1 = w^0 - \eta \frac{dL}{dw}|_{w=w^0}$。通过相反方向移动，令损失函数的值减小，从而达到我们的优化目的。此处的$\eta$是学习率。通过不断迭代，找到最终值。

<img src="/Users/LightningX/Learning/ML2020/2.Regression/Note/截屏2020-08-19 16.22.49.png" alt="截屏2020-08-19 16.22.49" style="zoom:33%;" />

注意，当参数变多的时候，计算梯度交替但独立，即在初值点得到不同参数的微分，对于所有参数使用当前的微分值更新，再计算所有更新后参数的新微分。

<img src="/Users/LightningX/Learning/ML2020/2.Regression/Note/截屏2020-08-19 16.28.54.png" alt="截屏2020-08-19 16.28.54" style="zoom:20%;" />

在以上的设定中，如果出现存在Local minima的函数，则优化过程与初值选取都有可能让搜寻陷入困境，然而，**线性回归不存在Local minima**。

#### How's the results?-Generalization

虽然训练的过程关心预测误差，但是实际上真正应该关心的是模型的泛化能力，即在测试数据上的表现。

***选择一个复杂的模型***

若考虑$y=b+w_1 \cdot x_{cp} + w_2 \cdot x_{cp}^2$...直至含有五次方项，随着每一次更新模型，在次数较小的时候，对测试数据的误差有可能变小。因此时高次模型实际上包含低次模型的所有函数（将高次项系数设为0即可），故寻找到一个使得训练数据误差最小的函数并非难事。然而，此时加入测试数据后，误差出现了巨大变化：

<img src="/Users/LightningX/Learning/ML2020/2.Regression/Note/截屏2020-08-19 16.56.49.png" alt="截屏2020-08-19 16.56.49" style="zoom:13%;" />

当模型复杂过头，就出现了过拟合Overfitting。

***一个启示***

在训练集上的误差下降并不说明什么，我们需要考虑测试数据的效果。

#### 另一种思路

***1.分类设计***

也许模型对于不同种类的Pokemon应该区别对待？

<img src="/Users/LightningX/Learning/ML2020/2.Regression/Note/截屏2020-08-19 17.10.22.png" alt="截屏2020-08-19 17.10.22" style="zoom:13%;" />

通过设计不同的激活权重，从而在遇到不同的Pokemon时触发不同的权重计算，注意此时仍然是一种线性模型。

***2.正则化***

为了避免模型过于复杂，可以加入正则化项$\lambda\sum(w_i)^2$。此时，损失函数受到该约束，因此会避免模型权重过大。此时，对数值做一个轻微扰动，因权重被限制，预测值$y = b + \sum w_i (x_i+\triangle x_i)$的变化也较小，即函数的图像更加平滑。

设置正则参数可以平衡预测误差和权重本身的比例。正则参数较小，则关注预测误差本身，反之关注权重的取值。当函数较平滑时，对噪声是不敏感的，此时的模型泛化能力更出色，然而过于平滑也会令预测值较差。

<img src="/Users/LightningX/Learning/ML2020/2.Regression/Note/截屏2020-08-19 17.24.41.png" alt="截屏2020-08-19 17.24.41" style="zoom:13%;" />