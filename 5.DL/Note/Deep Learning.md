

## Brief Introduction

### DL三步走：神经网络

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 20.21.13.png" alt="截屏2020-10-09 20.21.13" style="zoom:20%;" />

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 20.22.10.png" alt="截屏2020-10-09 20.22.10" style="zoom:20%;" />

神经网络指由一系列神经元组合而成的网络，而神经元则是一些加法函数与激活函数的组合。神经网络的“Deep”体现在网络层数的多（即hidden layer数目）。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 20.24.48.png" alt="截屏2020-10-09 20.24.48" style="zoom:20%;" />

仔细分析不难发现，神经网络对于输入数据与参数的计算方式可以写作矩阵形式如上，因此实际上多层神经网络就是一种矩阵运算的嵌套。最终该网络连接到输出层，给出指定数目的输出（例如分类问题中类的数目）。

### DL三步走：函数的好坏

神经网络并不依靠人为设计函数（但仍然针对不同的问题在不同的网络结构上有不同的表现），因此主要的优化重心放在损失函数的刻画上。整个训练过程实际上是定义某种合适的损失函数，并令其最小，从而达到函数效果最佳的目的。

### DL三步走：找出最好的函数

在规定衡量标准后，即可开始进行训练，这时候涉及到了先前提到的梯度下降方法：针对每一个参数，计算损失函数到其上的梯度，并更新原值。更进一步说，就是“反向传播”，这将在下一节被介绍到。

## Why Deep Learning？

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 20.36.03.png" alt="截屏2020-10-09 20.36.03" style="zoom:20%;" />

了解深度学习的基本概念后，有个疑问自然地浮现：深度学习真的是越深越好吗？上图表格以一个语音转换任务为例，每一层都有相同数目神经元的网络，随着层数增加，错误率在不断降低。但简单来说，这是一个极其复杂的函数，随着参数的增加，其对于同样的数据，刻画细节的能力理应有所提升。所以我们不禁关心，相同参数情况下的深浅不一的网络（深+瘦与浅+胖）会有什么样的不同：

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 20.58.54.png" alt="截屏2020-10-09 20.58.54" style="zoom:20%;" />

如图，同一行代表相同数目等级的参数，显然高瘦的网络准确率比矮胖的要好（关于这里头的参数数量计算，原论文未找到详情）。

### 模块化思维

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 21.07.17.png" alt="截屏2020-10-09 21.07.17" style="zoom:20%;" />

考虑一个单层网络，将图片输入后就能得到分类。如上图，分类器将直接对不同发型的不同性别人士作出分类，则对于某些数据较少的类，训练效果将不够好。而若将特征拆分，从一步到位的分类变成两个识别性别和发型的分类器组合，那么至少这两个基本分类器都会有足够的数据，从而能得到更好的表现。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-09 21.10.18.png" alt="截屏2020-10-09 21.10.18" style="zoom:20%;" />

实际上大众对于深度学习的主要印象还停留在“需要大量的训练数据”。但深度学习本身不应该如此。简单来说，如果数据多到每一个case都有对应的资料，那么一个查找表就可以解决所有的问题。***所以深度学习本质上还是从有限的数据之中找到其中的联系规律***，因此如何设计网络令其对少数据有更佳表现就是深度学习的一个目的了。

因为我们人为的对网络进行分层，所以每一个隐层实际上都相当于所谓的基分类器，逐层学到一部分特征，并互相组合，从而产生最后的预测结果。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 09.26.20.png" alt="截屏2020-10-10 09.26.20" style="zoom:20%;" />

以图像识别为例，广为人知的不同hidden unit输出图形实际上就是整个图像的一部分。从简单的边、线到复杂的图形，很显然深度足够的网络才能将其准确刻画。关于CNN网络的隐层单元输出，后续会介绍道。另一个类似的例子关于语音，此处不详述。

### Universality Theorem

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 09.29.27.png" alt="截屏2020-10-10 09.29.27" style="zoom:20%;" />

学者发现，任何连续函数在给出足够的隐层神经元情况下，一个单隐层网络即可进行拟合。首先这是对深度学习方法由来的神经网络效果之肯定。但是实际上更深的结构能够将隐层单元数量大大降低，以下由一个电路的例子进行说明。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 09.31.50.png" alt="截屏2020-10-10 09.31.50" style="zoom:20%;" />

类比神经网络，电路中的门类似于网络的神经元。电路理论中一个两层逻辑门可以表示任何布尔函数，这类比于单隐层网络可以表示任何连续函数。但更多层逻辑门可以让构造出的总体电路更加简单（门更少）。所以我们不禁认为更深的网络可以用更少的神经元来表示，从而需要更少的参数，简化网络。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 09.35.17.png" alt="截屏2020-10-10 09.35.17" style="zoom:20%;" />

如图，对于长度为2的输入AB，当其中1的个数为偶数时输出1，否则输出0。这样一个功能通过电路进行表示，当长度为d的时候，双层网络需要$O(2^d)$个门，但是多层网络用$O(d)$个门即可表示。如此，我们不难想象，随着层数增加，我们所用的单元数目是可以降低的，同时并不改变网络拟合一切的特性。

### End-to-end Learning

传统的机器学习方法中，设计算法仅仅是一小部分，很大程度上都依赖前期的特征处理。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 10.01.12.png" alt="截屏2020-10-10 10.01.12" style="zoom:20%;" />

同样以语音识别为例，传统方法取得声音波形后，利用多种不同的人工处理方式（绿色框），最终将得到的数据送入高斯混合模型，而这一步才是真正的令算法从数据中学习到内容。端到端学习的意义在于，将其中的所有需要专业知识的数据准备过程全部去除，从数据输入端到结果输出端，中间利用一堆黑盒连接，即可得到所需的结果。利用深度学习强大的拟合能力，可以大大简化学习过程。

## 反向传播

### 来源：梯度下降

神经网络的参数优化更新方式是梯度下降，反向传播实际上也是计算梯度的过程。

### 原理：链式法则

对于复合微分函数，有如下两个定理：

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 13.10.21.png" alt="截屏2020-10-10 13.10.21" style="zoom:20%;" />

上述被称为链式法则，我们后面所用到的所有微分计算均基于此。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 13.13.10.png" alt="截屏2020-10-10 13.13.10" style="zoom:20%;" />

反向传播实际上分为Forward pass与Backward pass两部分。将神经网络抽象为组合输入→激活函数→输出的结构，则前向过程就是计算所有组合输入z对于当前相关联的所有参数微分，这是不需要其他数据辅助的（例如这里对于z，计算w1、w2与b的微分）；而后向过程则是从损失函数开始，计算其对于这个激活函数输入z的微分。很显然，该过程需要经过激活函数后的权重，所以不能从前向后计算。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 13.17.02.png" alt="截屏2020-10-10 13.17.02" style="zoom:20%;" />

后向过程如上图，从损失函数计算对于组合输入z的微分时，要计算z经过激活函数并产生新的组合输入的微分，而计算这些组合输入微分，又需要计算更向后的组合输入z‘之微分，直到走到输出层。所以后向过程实际上是一个递归过程，并且只能从后往前，因此命名如是。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-10-10 14.51.05.png" alt="截屏2020-10-10 14.51.05" style="zoom:20%;" />

将前向过程和后向过程所求结果相乘，即可得到我们所需的损失函数对参数的梯度。

## Tips

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.06.02.png" alt="截屏2020-12-03 11.06.02" style="zoom:25%;" />

一般的深度学习流程如上图示，一般将认为数据表现不佳是模型、损失函数的问题，但实际上经过研究发现，单纯增加网络层数目并不会让模型表现更佳。

### Vanishing Gradient Problem

多层网络中，末层通过直接的反向传播，能够感知到梯度的变化，从而很快地进行学习，但前几层网络会因为传回的梯度太小而停留在一种原始未训练的状态。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.08.57.png" alt="截屏2020-12-03 11.08.57" style="zoom:25%;" />

使用Sigmoid函数的情况下，当输入的变化很大时，输出的变化会非常小，多层网络叠加使得变化不断衰减，从而对损失函数的影响很小，最后对gradient的贡献就小了。

#### ReLU

一种解决方式是对激活函数的改进，比如大家最常用的ReLU，具有计算快速、无梯度消失的优点，并且其实际上是许多bias不一样的sigmoid函数叠加。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.15.26.png" alt="截屏2020-12-03 11.15.26" style="zoom:25%;" />

通过max(0,z)的函数，输出为0的单元在网络中可以直接去除，从而网络剩下一个更简单的线性网络，这样的网络不会压缩梯度。但实际上因为函数值作较大改动的时候网络仍然是一堆“折线段”拼起来的，所以还是一个非线性网络。ReLU还有Leakey ReLU或带参数的版本。

#### Maxout

同时，如果想让激活函数也可被学习，我们就得到了Maxout函数：



<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.16.06.png" alt="截屏2020-12-03 11.16.06" style="zoom:25%;" />

通过将一组里头的最大值输出，我们就得到了几个函数的组合。Maxout的一种特殊例就是线性函数和0的组合，即ReLU。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.24.02.png" alt="截屏2020-12-03 11.24.02" style="zoom:25%;" />

越来越多的函数在一组内合并，实际上是求取它们的驻点上确界，是一个片段线性的组合凸函数。

### Early Stopping

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.28.37.png" alt="截屏2020-12-03 11.28.37" style="zoom:25%;" />

训练中有可能在某个阶段对Validation set的结果已经达到了最佳，继续训练会产生过拟合，所以要提前停止。

### Regularization

对于函数$L(\theta)$，如果加入L2正则项，表示为$L(\theta)+\frac{\lambda}{2}||\theta||_2$，其梯度为$\frac{\partial L}{\partial w}+\lambda w$，对于更新过程有：
$$
\begin{align*}
w^{t+1} &=w^t-\eta(\frac{\partial L}{\partial w} + \lambda w^t)
\\
&=(1-\eta\lambda)w^t-\eta\frac{\partial L}{\partial w}
\end{align*}
$$
其中$(1-\eta\lambda)w^t$的系数实际上是一个接近1的数字，所以不会真的让参数变小太多。但因为它仍然具有这个作用，所以被称作“Weight decay”。

对于L1正则项$L(\theta)+\frac{\lambda}{2}||\theta||_1$，梯度为$\frac{\partial L}{\partial w}+\lambda sgn(w)$，更新过程为：
$$
\begin{align*}
w^{t+1} &=w^t-\eta(\frac{\partial L}{\partial w} + \lambda sgn(w^t))
\\
&=w^t-\eta\frac{\partial L}{\partial w}-\eta\lambda sgn(w^t)
\end{align*}
$$
通过L1正则项的训练，每次下降的时候，都额外下降一个固定的值（或上升），而非L2的大值下降更快，所以L1正则化的训练仍然保留较大的权重，对小权重则正常减小，从而产生比较稀疏的结果（0附近的结果多）。

### Dropout

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.43.23.png" alt="截屏2020-12-03 11.43.23" style="zoom:25%;" />

如果在训练时，对网络里头的神经元做采样，从而将p%的神经元丢弃，那么我们的网络实际上会变成一个细长的样子。但我们在每次更新参数的时候都会重新做一次随机采样，所以网络的结果将不断变化！但注意，对于测试结果，需要用上所有神经元，并且乘上权重的系数。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.45.17.png" alt="截屏2020-12-03 11.45.17" style="zoom:25%;" />

一种直观的解释是，因为这次训练的时候失去了部分神经元，所以留下来的神经元就需要做到更多结果，所以大家互相变强。

最后使用的时候因为所有权重都用到，所以输出的结果是有倍数的，所以需要乘上系数。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.47.22.png" alt="截屏2020-12-03 11.47.22" style="zoom:25%;" />

从另一个角度来看，Dropout实际上是增加网络的鲁棒性，它的本质就是一种集成模型的学习。所有结果通过乘上系数进行组合，相当于对所有结果投票平均，只要大部分模型的判定正确，结果的判定就能正确。对于M个神经元，每个都有可能被drop，所以实际上有$2^M$个网络，这样的集成模型就极其强大了。又因为参数之间是共享的，所以这样的训练并不会带来太大的压力，却提供了很好的测试集上结果。

<img src="/Users/LightningX/Learning/ML2020/5.DL/Note/截屏2020-12-03 11.48.53.png" alt="截屏2020-12-03 11.48.53" style="zoom:25%;" />